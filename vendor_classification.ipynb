{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949f7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.keras.layers import Input, Embedding, Concatenate, LSTM, Bidirectional, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183f89bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_vendor_dataset = pd.read_json('datasets/dataset_1500_seq_length.json')\n",
    "dataset_list = []\n",
    "for _, row in anon_vendor_dataset.iterrows():\n",
    "    profile_data = row['profile']\n",
    "    if isinstance(profile_data, str):\n",
    "        profile_data = json.loads(profile_data)\n",
    "    dataset_list.append({\"profile\": profile_data, \"label\": row['label']})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bff55d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75919eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfilePreprocessor:\n",
    "    \"\"\"\n",
    "    A pipeline to preprocess and encode sequential profile data.\n",
    "\n",
    "    This class handles:\n",
    "    - Integer encoding for finite string variables (behavior, method, network_method).\n",
    "    - Top-K encoding for string variables (element_tagname, event_handled),\n",
    "      mapping less frequent values to an 'other' category.\n",
    "    - Boolean to integer conversion for 'contains_sensitive_data'.\n",
    "    - Retention of float values for 'delay'.\n",
    "    Each categorical feature maintains its own separate vocabulary and mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k_elements=10, top_k_events=10):\n",
    "        \"\"\"\n",
    "        Initializes the preprocessor with parameters for top-K encoding.\n",
    "\n",
    "        Args:\n",
    "            top_k_elements (int): The number of top most frequent element_tagname\n",
    "                                  values to keep. Others will be mapped to 'other'.\n",
    "            top_k_events (int): The number of top most frequent event_handled\n",
    "                                values to keep. Others will be mapped to 'other'.\n",
    "        \"\"\"\n",
    "        self.top_k_elements = top_k_elements\n",
    "        self.top_k_events = top_k_events\n",
    "        self.behavior_mapping = {}\n",
    "        self.method_mapping = {}\n",
    "        self.element_tagname_mapping = {}\n",
    "        self.event_handled_mapping = {}\n",
    "        self.label_mapping = {} \n",
    "        self._next_behavior_id_ref = [1]\n",
    "        self._next_method_id_ref = [1]\n",
    "        self._next_element_tagname_id_ref = [1]\n",
    "        self._next_event_handled_id_ref = [1]\n",
    "        self._next_label_id_ref = [0]\n",
    "        self.max_len = 0\n",
    "        self._element_tagname_counts = defaultdict(int)\n",
    "        self._event_handled_counts = defaultdict(int)\n",
    "\n",
    "    def _get_or_assign_id(self, value, mapping, next_id_ref):\n",
    "        \"\"\"Helper to get ID or assign a new one for a specific categorical mapping.\"\"\"\n",
    "        if value not in mapping:\n",
    "            mapping[value] = next_id_ref[0]\n",
    "            next_id_ref[0] += 1\n",
    "        return mapping[value]\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"\n",
    "        Fits the preprocessor to the dataset to learn all necessary mappings.\n",
    "\n",
    "        Iterates through all profiles and their events to collect unique values\n",
    "        and frequency counts for various fields.\n",
    "\n",
    "        Args:\n",
    "            dataset (list): A list of dictionaries, where each dictionary\n",
    "                            has \"profile\" (list of event objects) and \"label\".\n",
    "        \"\"\"\n",
    "\n",
    "        for data_point in dataset:\n",
    "            profile = data_point[\"profile\"]\n",
    "            if self.max_len < len(profile):\n",
    "                self.max_len = len(profile)\n",
    "\n",
    "            self._get_or_assign_id(data_point[\"label\"], self.label_mapping, self._next_label_id_ref)\n",
    "            for event in profile:\n",
    "                self._get_or_assign_id(event[\"behavior\"], self.behavior_mapping, self._next_behavior_id_ref)\n",
    "                self._get_or_assign_id(event[\"method\"], self.method_mapping, self._next_method_id_ref)\n",
    "                self._element_tagname_counts[event[\"element_tagname\"]] += 1\n",
    "                self._event_handled_counts[event[\"event_handled\"]] += 1\n",
    "\n",
    "        sorted_elements = sorted(self._element_tagname_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "        sorted_events = sorted(self._event_handled_counts.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        for i, (tag, _) in enumerate(sorted_elements):\n",
    "            if i < self.top_k_elements:\n",
    "                self._get_or_assign_id(tag, self.element_tagname_mapping, self._next_element_tagname_id_ref)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        self._get_or_assign_id(\"__OTHER_TAG__\", self.element_tagname_mapping, self._next_element_tagname_id_ref)\n",
    "        for i, (event_name, _) in enumerate(sorted_events):\n",
    "            if i < self.top_k_events:\n",
    "                self._get_or_assign_id(event_name, self.event_handled_mapping, self._next_event_handled_id_ref)\n",
    "            else:\n",
    "                break \n",
    "        self._get_or_assign_id(\"__OTHER_EVENT__\", self.event_handled_mapping, self._next_event_handled_id_ref)\n",
    "\n",
    "        self._get_or_assign_id(\"__UNKNOWN__\", self.behavior_mapping, self._next_behavior_id_ref)\n",
    "        self._get_or_assign_id(\"__UNKNOWN__\", self.method_mapping, self._next_method_id_ref)\n",
    "\n",
    "        print(\"Fitting complete. Learned mappings:\")\n",
    "        print(f\"Behavior mapping: {self.behavior_mapping}\")\n",
    "        print(f\"Method mapping: {self.method_mapping}\")\n",
    "        print(f\"Element Tagname mapping (top {self.top_k_elements} + other): {self.element_tagname_mapping}\")\n",
    "        print(f\"Event Handled mapping (top {self.top_k_events} + other): {self.event_handled_mapping}\")\n",
    "        print(f\"Label mapping: {self.label_mapping}\")\n",
    "\n",
    "    def transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Transforms the dataset using the learned mappings.\n",
    "\n",
    "        Args:\n",
    "            dataset (list): A list of dictionaries, where each dictionary\n",
    "                            has \"profile\" (list of event objects) and \"label\".\n",
    "\n",
    "        Returns:\n",
    "            list: A new list of dictionaries with transformed profiles. Each\n",
    "                  event in a profile is now a list of numerical features.\n",
    "        \"\"\"\n",
    "\n",
    "        transformed_dataset = []\n",
    "        for data_point in dataset:\n",
    "            transformed_profile = []\n",
    "            profile = data_point[\"profile\"]\n",
    "            encoded_label = self.label_mapping.get(data_point[\"label\"], -1) # -1 for unknown label\n",
    "            for event in profile:\n",
    "    \n",
    "                encoded_behavior = self.behavior_mapping.get(\n",
    "                    event.get(\"behavior\"),\n",
    "                    self.behavior_mapping[\"__UNKNOWN__\"]\n",
    "                )\n",
    "                encoded_method = self.method_mapping.get(\n",
    "                    event.get(\"method\"),\n",
    "                    self.method_mapping[\"__UNKNOWN__\"]\n",
    "                )\n",
    "\n",
    "                encoded_element_tagname = self.element_tagname_mapping.get(\n",
    "                    event.get(\"element_tagname\"), self.element_tagname_mapping[\"__OTHER_TAG__\"]\n",
    "                )\n",
    "                encoded_event_handled = self.event_handled_mapping.get(\n",
    "                    event.get(\"event_handled\"), self.event_handled_mapping[\"__OTHER_EVENT__\"]\n",
    "                )\n",
    "\n",
    "\n",
    "                transformed_profile.append([\n",
    "                    encoded_behavior,\n",
    "                    encoded_method,\n",
    "                    encoded_element_tagname,\n",
    "                    encoded_event_handled,\n",
    "                ])\n",
    "            obj = {\n",
    "                \"profile\": transformed_profile,\n",
    "                \"label\": encoded_label\n",
    "            }\n",
    "            if 'version' in data_point:\n",
    "                obj['version'] = data_point['version']\n",
    "            transformed_dataset.append(obj)\n",
    "\n",
    "        print(\"Transformation complete.\")\n",
    "        return transformed_dataset\n",
    "\n",
    "    def fit_transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Fits the preprocessor and then transforms the dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (list): A list of dictionaries, where each dictionary\n",
    "                            has \"profile\" (list of event objects) and \"label\".\n",
    "\n",
    "        Returns:\n",
    "            list: A new list of dictionaries with transformed profiles.\n",
    "        \"\"\"\n",
    "\n",
    "        self.fit(dataset)\n",
    "        return self.transform(dataset)\n",
    "\n",
    "    @property\n",
    "    def behavior_vocab_size(self):\n",
    "        \"\"\"Returns the vocabulary size for the 'behavior' mapping.\"\"\"\n",
    "        return self._next_behavior_id_ref[0]\n",
    "\n",
    "    @property\n",
    "    def method_vocab_size(self):\n",
    "        \"\"\"Returns the vocabulary size for the 'method' mapping.\"\"\"\n",
    "        return self._next_method_id_ref[0]\n",
    "    \n",
    "    @property\n",
    "    def element_tagname_vocab_size(self):\n",
    "        \"\"\"Returns the vocabulary size for the 'element_tagname' mapping.\"\"\"\n",
    "        return self._next_element_tagname_id_ref[0]\n",
    "\n",
    "    @property\n",
    "    def event_handled_vocab_size(self):\n",
    "        \"\"\"Returns the vocabulary size for the 'event_handled' mapping.\"\"\"\n",
    "        return self._next_event_handled_id_ref[0]\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        \"\"\"Returns the number of unique labels.\"\"\"\n",
    "        return self._next_label_id_ref[0]\n",
    "\n",
    "def prepare_model_inputs(processed_dataset, versions=False, max_len=None):\n",
    "    \"\"\"\n",
    "    Prepares the processed dataset into a format suitable for the Keras multi-input model.\n",
    "\n",
    "    Args:\n",
    "        processed_dataset (list): The output from ProfilePreprocessor.transform().\n",
    "                                  A list of dictionaries, where each dict has\n",
    "                                  \"profile\" (list of numerical features) and \"label\" (encoded int).\n",
    "        max_len (int, optional): The maximum sequence length to pad to. If None,\n",
    "                                 it will be determined from the longest profile in the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - inputs_dict (dict): A dictionary of NumPy arrays, where keys are\n",
    "                                  input layer names and values are the padded feature sequences.\n",
    "            - labels (np.ndarray): A NumPy array of the encoded labels.\n",
    "    \"\"\"\n",
    "\n",
    "    if not processed_dataset:\n",
    "        return {}, np.array([]), np.array([])\n",
    "\n",
    "    if max_len is None:\n",
    "        max_len = max(len(data_point[\"profile\"]) for data_point in processed_dataset)\n",
    "        print(f\"Determined max_len from dataset: {max_len}\")\n",
    "    else:\n",
    "        print(f\"Using provided max_len: {max_len}\")\n",
    "\n",
    "    behavior_sequences = []\n",
    "    method_sequences = []\n",
    "    element_tagname_sequences = []\n",
    "    event_handled_sequences = []\n",
    "    labels = []\n",
    "    version_labels = []\n",
    "\n",
    "    for data_point in processed_dataset:\n",
    "        profile_data = np.array(data_point[\"profile\"]) \n",
    "        original_len = profile_data.shape[0]\n",
    "        if max_len and original_len > max_len:\n",
    "            front_len = max_len // 2\n",
    "            back_len = max_len - front_len\n",
    "            \n",
    "            front_part = profile_data[:front_len, :]\n",
    "            back_part = profile_data[-back_len:, :]\n",
    "\n",
    "            profile_data = np.concatenate((front_part, back_part), axis=0)\n",
    "\n",
    "        if \"label\" in data_point:\n",
    "            labels.append(data_point[\"label\"])\n",
    "        if versions:\n",
    "          version_labels.append(data_point[\"version\"])\n",
    "\n",
    "        if profile_data.size > 0:\n",
    "          behavior_sequences.append(profile_data[:, 0])\n",
    "          method_sequences.append(profile_data[:, 1])\n",
    "          element_tagname_sequences.append(profile_data[:, 2])\n",
    "          event_handled_sequences.append(profile_data[:, 3])\n",
    "\n",
    "        else:\n",
    "          behavior_sequences.append([])\n",
    "          method_sequences.append([])\n",
    "          element_tagname_sequences.append([])\n",
    "          event_handled_sequences.append([])\n",
    "\n",
    "    padded_behavior = pad_sequences(behavior_sequences, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n",
    "    padded_method = pad_sequences(method_sequences, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n",
    "    padded_element_tagname = pad_sequences(element_tagname_sequences, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n",
    "    padded_event_handled = pad_sequences(event_handled_sequences, maxlen=max_len, padding='post', truncating='post', dtype='int32')\n",
    "\n",
    "    inputs_dict = {\n",
    "        'behavior_input': padded_behavior,\n",
    "        'method_input': padded_method,\n",
    "        'element_tagname_input': padded_element_tagname,\n",
    "        'event_handled_input': padded_event_handled,\n",
    "    }\n",
    "\n",
    "    return inputs_dict, np.array(labels), np.array(version_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f4f31",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f1fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.keras.utils.register_keras_serializable()\n",
    "class SupervisedContrastiveLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Implements the Supervised Contrastive Loss.\n",
    "\n",
    "    This loss encourages embeddings of samples with the same label to be closer,\n",
    "    and embeddings of samples with different labels to be further apart.\n",
    "\n",
    "    Key points:\n",
    "    - Embeddings are L2-normalized before computing pairwise similarities.\n",
    "    - Similarity is computed via dot-product divided by a temperature parameter.\n",
    "    - Positive pairs are samples that share the same label.\n",
    "    - Uses a mask to ensure a sample is not paired with itself.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature=0.1, reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE, name='SupConLoss'):\n",
    "        \"\"\"\n",
    "        Initializes the supervised contrastive loss.\n",
    "\n",
    "        Args:\n",
    "            temperature (float): Temperature scaling factor for contrastive logits.\n",
    "            reduction: How the final loss is reduced over the batch.\n",
    "            name (str): Name of the loss object.\n",
    "        \"\"\"\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def call(self, labels, embeddings):\n",
    "        \"\"\"\n",
    "        Computes the supervised contrastive loss.\n",
    "\n",
    "        Args:\n",
    "            labels (Tensor): Encoded integer labels of shape (batch,).\n",
    "            embeddings (Tensor): Embedding vectors of shape (batch, dim).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = tf.cast(tf.reshape(labels, [-1, 1]), tf.int32)\n",
    "        embeddings = tf.math.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "        sim = tf.matmul(embeddings, embeddings, transpose_b=True) / self.temperature\n",
    "        mask = tf.cast(tf.equal(labels, tf.transpose(labels)), tf.float32)\n",
    "        mask_self = tf.eye(tf.shape(labels)[0])\n",
    "        mask = mask - mask_self\n",
    "\n",
    "        logits_max = tf.reduce_max(sim, axis=1, keepdims=True)\n",
    "        logits = sim - logits_max\n",
    "\n",
    "        exp_logits = tf.exp(logits) * (1 - mask_self)\n",
    "        log_prob = logits - tf.math.log(tf.reduce_sum(exp_logits, axis=1, keepdims=True) + 1e-9)\n",
    "\n",
    "        mean_log_prob_pos = tf.reduce_sum(mask * log_prob, axis=1) / (tf.reduce_sum(mask, axis=1) + 1e-9)\n",
    "        loss = -tf.reduce_mean(mean_log_prob_pos)\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"Enables saving/loading through Keras serialization.\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"temperature\": self.temperature,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class L2Normalize(Layer):\n",
    "    \"\"\"\n",
    "    A simple Keras Layer that performs L2 normalization on the last axis.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        return tf.math.l2_normalize(inputs, axis=-1)\n",
    "\n",
    "\n",
    "def get_embedding_model_multi_input(input_embedding_dim, final_embedding_dim, vocab_sizes, max_len, bidirectional=True):\n",
    "    \"\"\"\n",
    "    Builds a multi-input embedding model for sequential categorical features.\n",
    "\n",
    "    This model:\n",
    "    - Creates an embedding layer per feature (behavior/method/...).\n",
    "    - Concatenates all embeddings along the feature axis.\n",
    "    - Applies an LSTM (optionally bidirectional) to produce a fixed-length embedding.\n",
    "    - Outputs an L2-normalized embedding vector.\n",
    "\n",
    "    Args:\n",
    "        input_embedding_dim (int): Embedding dimension for each input feature.\n",
    "        final_embedding_dim (int): Output embedding dimension of the LSTM.\n",
    "        vocab_sizes (dict): Mapping: feature_name -> vocabulary_size.\n",
    "        max_len (int): Maximum sequence length.\n",
    "        bidirectional (bool): Whether to use a bidirectional LSTM.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (model, input_layers, output_tensor)\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = {\n",
    "        name: Input(shape=(max_len,), name=f\"{name}_input\", dtype='int32')\n",
    "        for name in vocab_sizes\n",
    "    }\n",
    "    embeddings = []\n",
    "    for name, vocab_size in vocab_sizes.items():\n",
    "        embeddings.append(\n",
    "            Embedding(\n",
    "                input_dim=vocab_size,\n",
    "                output_dim=input_embedding_dim,\n",
    "                mask_zero=True if name == \"behavior\" else False,\n",
    "                name=f\"{name}_embedding\"\n",
    "            )(inputs[name])\n",
    "        )\n",
    "\n",
    "    concatenated = Concatenate(axis=-1, name=\"concat_embeddings\")(embeddings)\n",
    "\n",
    "    mask = embeddings[0]._keras_mask \n",
    "\n",
    "    lstm = LSTM(final_embedding_dim, name=\"pool\")\n",
    "    if bidirectional:\n",
    "        lstm = Bidirectional(lstm)\n",
    "    x = lstm(concatenated, mask=mask)\n",
    "\n",
    "    normalized_output = L2Normalize(name='out')(x)\n",
    "\n",
    "    model = Model(inputs=list(inputs.values()), outputs=normalized_output, name=\"multi_input_embedding_model\")\n",
    "\n",
    "    return model, list(inputs.values()), normalized_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df2101",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4336bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(X_dict, y, middle=False, min_len=10):\n",
    "    \"\"\"\n",
    "    Data augmentation for sequential categorical features.\n",
    "\n",
    "    Strategy:\n",
    "    - Randomly choose a shorter length (>= min_len).\n",
    "    - Truncate the sequence either from the middle or the end.\n",
    "    - Pad back to full length.\n",
    "    - Duplicate labels accordingly.\n",
    "\n",
    "    Args:\n",
    "        X_dict (dict): Feature_name -> padded sequences.\n",
    "        y (Array): Labels.\n",
    "        middle (bool): Whether to truncate using (front + back) slicing.\n",
    "        min_len (int): Minimum allowed truncated length.\n",
    "\n",
    "    Returns:\n",
    "        (X_combined, y_combined): Original + augmented data.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = len(y)\n",
    "    feature_names = list(X_dict.keys())\n",
    "    seq_len = next(iter(X_dict.values())).shape[1]\n",
    "\n",
    "    if min_len >= seq_len:\n",
    "        raise ValueError(f\"min_len ({min_len}) must be smaller than sequence length ({seq_len})\")\n",
    "\n",
    "    X_aug = {feat: [] for feat in feature_names}\n",
    "    y_aug = []\n",
    "\n",
    "    for i in range(n_samples):\n",
    "      main_seq = X_dict['behavior_input'][i]\n",
    "      main_non_pad = main_seq[main_seq != 0]\n",
    "      len_non_pad = len(main_non_pad)\n",
    "      new_len = np.random.randint(min_len, len_non_pad)\n",
    "      new_len_2 = np.random.randint(min_len, len_non_pad)\n",
    "      if new_len_2 == new_len:\n",
    "        new_len_2 = np.random.randint(min_len, len_non_pad)\n",
    "        if new_len_2 == new_len:\n",
    "          new_len_2 = 0\n",
    "      for feat in feature_names:\n",
    "          seq = X_dict[feat][i]\n",
    "          non_pad = seq[seq != 0]\n",
    "\n",
    "          if len(non_pad) == 0:\n",
    "\n",
    "              truncated_1 = np.zeros(seq_len, dtype=seq.dtype)\n",
    "              truncated_2 = np.zeros(seq_len, dtype=seq.dtype)\n",
    "          else:\n",
    "            \n",
    "              if middle:\n",
    "                front_len_1 = new_len // 2\n",
    "                back_len_1 = new_len - front_len_1\n",
    "                front_part_1 = non_pad[:front_len_1]\n",
    "                back_part_1 = non_pad[-back_len_1:]\n",
    "                truncated_1 = np.concatenate((front_part_1, back_part_1))\n",
    "                front_len_2 = new_len_2 // 2\n",
    "                back_len_2 = new_len_2 - front_len_2\n",
    "                front_part_2 = non_pad[:front_len_2]\n",
    "                back_part_2 = non_pad[-back_len_2:]\n",
    "                truncated_2 = np.concatenate((front_part_2, back_part_2))\n",
    "              elif new_len_2:\n",
    "                truncated_1 = non_pad[-new_len:]\n",
    "                truncated_2 = non_pad[-new_len_2:]\n",
    "\n",
    "              if len(truncated_1) < seq_len:\n",
    "                  truncated_1 = np.pad(truncated_1, (0, seq_len - len(truncated_1)), constant_values=0)\n",
    "              if len(truncated_2) < seq_len:\n",
    "                  truncated_2 = np.pad(truncated_2, (0, seq_len - len(truncated_2)), constant_values=0)\n",
    "\n",
    "          X_aug[feat].append(truncated_1)\n",
    "          X_aug[feat].append(truncated_2)\n",
    "      y_aug.append(y[i])\n",
    "      y_aug.append(y[i])\n",
    "\n",
    "    X_aug = {feat: np.array(vals, dtype=X_dict[feat].dtype) for feat, vals in X_aug.items()}\n",
    "    y_aug = np.array(y_aug, dtype=y.dtype)\n",
    "\n",
    "    X_combined = {feat: np.concatenate([X_dict[feat], X_aug[feat]], axis=0) for feat in feature_names}\n",
    "    y_combined = np.concatenate([y, y_aug], axis=0)\n",
    "\n",
    "    return X_combined, y_combined\n",
    "\n",
    "\n",
    "def balance_data(X_dict, y, min_points=10, min_len=10, middle=False):\n",
    "    \"\"\"\n",
    "    Balances dataset classes by augmenting underrepresented classes.\n",
    "\n",
    "    Strategy:\n",
    "    - Identify labels with fewer than `min_points` samples.\n",
    "    - Augment them using truncation (similar to augment_data).\n",
    "    - Ensures balanced class distribution.\n",
    "\n",
    "    Args:\n",
    "        X_dict (dict): Feature_name -> padded sequences.\n",
    "        y (array): Label array.\n",
    "        min_points (int): Minimum number of samples required per class.\n",
    "        min_len (int): Minimum sequence length after truncation.\n",
    "        middle (bool): Whether to use middle truncation instead of prefix slicing.\n",
    "\n",
    "    Returns:\n",
    "        (X_combined, y_combined): Balanced dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_samples = len(y)\n",
    "    feature_names = list(X_dict.keys())\n",
    "    seq_len = next(iter(X_dict.values())).shape[1]\n",
    "\n",
    "    a = pd.Series(y).value_counts()\n",
    "    a = a[a < min_points]\n",
    "    labels, counts = list(a.index), list(a)\n",
    "    y_series = pd.Series(y)\n",
    "    X_aug = {feat: [] for feat in feature_names}\n",
    "    y_aug = []\n",
    "    for label, count in zip(labels, counts):\n",
    "        to_add = min_points - count\n",
    "\n",
    "        indexes = list(y_series[y_series == label].index)\n",
    "\n",
    "        if to_add < count:\n",
    "            sampled_indexes = sample(indexes, to_add)\n",
    "            for i in sampled_indexes:\n",
    "                main_seq = X_dict['behavior_input'][i]\n",
    "                main_non_pad = main_seq[main_seq != 0]\n",
    "                len_non_pad = len(main_non_pad)\n",
    "                truncable_len = len_non_pad - min_len\n",
    "                if truncable_len:\n",
    "                    to_trunc_len = np.random.randint(1, min(truncable_len + 1, min_len))\n",
    "                    new_len = len_non_pad - to_trunc_len\n",
    "\n",
    "                    for feat in feature_names:\n",
    "                        seq = X_dict[feat][i]\n",
    "                        non_pad = seq[seq != 0]\n",
    "                        if middle:\n",
    "                            front_len = new_len // 2\n",
    "                            back_len = new_len - front_len\n",
    "                            front_part = non_pad[:front_len]\n",
    "                            back_part = non_pad[-back_len:]\n",
    "                            truncated = np.concatenate((front_part, back_part))\n",
    "                        else:\n",
    "                            truncated = non_pad[:new_len]\n",
    "                        if len(truncated) < seq_len:\n",
    "                            truncated = np.pad(truncated, (0, seq_len - len(truncated)), constant_values=0)\n",
    "                        else:\n",
    "                            truncated = []\n",
    "                        if len(truncated) == seq_len:\n",
    "                          X_aug[feat].append(truncated)\n",
    "\n",
    "                    y_aug.append(y[i])\n",
    "        else:\n",
    "            pulls_from_index, remaining_pulls = to_add // count, to_add % count\n",
    "            for i in indexes:\n",
    "                main_seq = X_dict['behavior_input'][i]\n",
    "                main_non_pad = main_seq[main_seq != 0]\n",
    "                len_non_pad = len(main_non_pad)\n",
    "                truncable_len = len_non_pad - min_len\n",
    "                to_trunc_lens = []\n",
    "                while truncable_len > 0 and len(to_trunc_lens) < pulls_from_index:\n",
    "                    to_trunc_len = np.random.randint(1, 4)\n",
    "                    if to_trunc_lens:\n",
    "                        to_trunc_len += to_trunc_lens[-1]\n",
    "                    if truncable_len - to_trunc_len >= min_len:\n",
    "                        to_trunc_lens.append(to_trunc_len)\n",
    "                    truncable_len -= to_trunc_len\n",
    "                if remaining_pulls:\n",
    "                    to_trunc_len = np.random.randint(1, 3)\n",
    "                    if to_trunc_lens:\n",
    "                        to_trunc_len += to_trunc_lens[-1]\n",
    "                    if truncable_len - to_trunc_len >= min_len:\n",
    "                        to_trunc_lens.append(to_trunc_len)\n",
    "                for ttl in to_trunc_lens:\n",
    "                    new_len = len_non_pad - ttl\n",
    "                    for feat in feature_names:\n",
    "                        seq = X_dict[feat][i]\n",
    "                        non_pad = seq[seq != 0]\n",
    "                        if middle:\n",
    "                            front_len = new_len // 2\n",
    "                            back_len = new_len - front_len\n",
    "                            front_part = non_pad[:front_len]\n",
    "                            back_part = non_pad[-back_len:]\n",
    "                            truncated = np.concatenate((front_part, back_part))\n",
    "                        else:\n",
    "                            truncated = non_pad[:new_len]\n",
    "\n",
    "                        if len(truncated) < seq_len:\n",
    "                            truncated = np.pad(truncated, (0, seq_len - len(truncated)), constant_values=0)\n",
    "                        else:\n",
    "                            truncated = []\n",
    "\n",
    "                        if len(truncated) == seq_len:\n",
    "                          X_aug[feat].append(truncated)\n",
    "\n",
    "                    y_aug.append(y[i])\n",
    "\n",
    "    X_aug = {feat: np.array(vals, dtype=X_dict[feat].dtype) for feat, vals in X_aug.items()}\n",
    "    y_aug = np.array(y_aug, dtype=y.dtype)\n",
    "    \n",
    "    X_combined = {feat: np.concatenate([X_dict[feat], X_aug[feat]], axis=0) for feat in feature_names}\n",
    "    y_combined = np.concatenate([y, y_aug], axis=0)\n",
    "\n",
    "    return X_combined, y_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f6da9a",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cfc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "patience = 10 \n",
    "augment = 'middle'\n",
    "balance = False\n",
    "top_k_events = 90\n",
    "top_k_elements = 42\n",
    "early_stop_start = 20\n",
    "unknown_vendor_threshold = 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02da02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_retrieve_acc(emb_train, y_train, emb_test, y_test, k=1, unknown_threshold=0):\n",
    "    \"\"\"\n",
    "    Computes top-K retrieval accuracy using cosine similarity between embeddings.\n",
    "\n",
    "    This function evaluates how well test embeddings retrieve the correct label\n",
    "    from the training set based on similarity ranking.\n",
    "\n",
    "    Process:\n",
    "    - Compute cosine similarity between every test embedding and all train embeddings.\n",
    "    - For each test sample, retrieve the top-K most similar training samples.\n",
    "    - Check whether the correct label appears among the top-K predictions.\n",
    "    - Optionally apply an unknown-threshold: only count a hit if the similarity\n",
    "      for the matching label is above the threshold.\n",
    "\n",
    "    Args:\n",
    "        emb_train (np.ndarray): Training embeddings, shape (N_train, dim).\n",
    "        y_train (array-like): Labels for training embeddings.\n",
    "        emb_test (np.ndarray): Test embeddings, shape (N_test, dim).\n",
    "        y_test (array-like): Labels for test embeddings.\n",
    "        k (int): Number of nearest neighbors to retrieve.\n",
    "        unknown_threshold (float): Minimum cosine similarity required to count a\n",
    "                                   match as correct. If 0, thresholding is disabled.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - hit_rate (float): Top-K retrieval accuracy.\n",
    "            - top_k_predictions (np.ndarray): Retrieved labels for each test sample,\n",
    "                                              shape (N_test, k).\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(y_train, np.ndarray):\n",
    "        y_train = np.array(y_train)\n",
    "\n",
    "    sim_matrix = cosine_similarity(emb_test, emb_train)\n",
    "\n",
    "    top_k_indices = np.argsort(sim_matrix, axis=1)[:, -k:]\n",
    "    top_k_indices = np.fliplr(top_k_indices)\n",
    "\n",
    "    true_labels = y_test\n",
    "    top_k_predictions = y_train[top_k_indices]\n",
    "    top_k_similarities = np.take_along_axis(sim_matrix, top_k_indices, axis=1)\n",
    "    hit_count = 0\n",
    "    for i in range(len(true_labels)):\n",
    "        true_label = true_labels[i]\n",
    "        if true_label in top_k_predictions[i]:\n",
    "            if unknown_threshold == 0:\n",
    "                hit_count += 1\n",
    "            else:\n",
    "                match_indices_in_k = np.where(top_k_predictions[i] == true_label)[0]\n",
    "                best_match_index_in_k = match_indices_in_k[0]\n",
    "                match_similarity = top_k_similarities[i, best_match_index_in_k]\n",
    "                if match_similarity >= unknown_threshold:\n",
    "                    hit_count += 1\n",
    "\n",
    "    hit_rate = hit_count / len(y_test)\n",
    "\n",
    "    return hit_rate, top_k_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e3ca7",
   "metadata": {},
   "source": [
    "## Vendor Identification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d023ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform stratified k-fold cross-validation.\n",
    "\n",
    "StratifiedKFold creates multiple train/test splits while preserving the\n",
    "class distribution of the labels. This ensures each fold has similar \n",
    "label proportions, which is important for fair evaluation.\n",
    "\n",
    "Inside the loop, we:\n",
    "    1. Build train/test subsets using these indices\n",
    "    2. Fit the preprocessor on training data only\n",
    "    3. Transform both train and test sets\n",
    "    4. Train the embedding model\n",
    "    5. Generate embeddings for retrieval\n",
    "    6. Compute top-K retrieval accuracy for the fold\n",
    "\"\"\"\n",
    "\n",
    "hit_rates = []\n",
    "\n",
    "raw_labels = [dp[\"label\"] for dp in dataset_list]\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(np.arange(len(dataset_list)), raw_labels), 1):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    train_raw = [dataset_list[i] for i in train_idx]\n",
    "    test_raw  = [dataset_list[i] for i in test_idx]\n",
    "\n",
    "    preprocessor = ProfilePreprocessor(top_k_elements=42,\n",
    "                                        top_k_events=90)\n",
    "    train_processed = preprocessor.fit_transform(train_raw)\n",
    "    test_processed  = preprocessor.transform(test_raw)\n",
    "\n",
    "\n",
    "    MAX_SEQUENCE_LENGTH = max(len(dp[\"profile\"]) for dp in train_processed)\n",
    "    X_train, y_train, _ = prepare_model_inputs(train_processed, max_len=MAX_SEQUENCE_LENGTH)\n",
    "    X_test, y_test, _   = prepare_model_inputs(test_processed, max_len=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "    vocab_sizes = {\n",
    "        'behavior': preprocessor.behavior_vocab_size,\n",
    "        'method': preprocessor.method_vocab_size,\n",
    "        'element_tagname': preprocessor.element_tagname_vocab_size,\n",
    "        'event_handled': preprocessor.event_handled_vocab_size,\n",
    "    }\n",
    "\n",
    "    if augment:\n",
    "        print('Augmenting')\n",
    "        X_train_aug, y_train_aug = augment_data(X_train, y_train, middle=augment == 'middle', min_len=5)\n",
    "    elif balance:\n",
    "        print('Balancing')\n",
    "        X_train_aug, y_train_aug = balance_data(X_train, y_train, min_points=10, min_len=5, middle=balance == 'middle')\n",
    "    else:\n",
    "        X_train_aug, y_train_aug = X_train, y_train\n",
    "\n",
    "    indices = np.arange(len(y_train_aug))\n",
    "    embed_train_idx, embed_val_idx = train_test_split(\n",
    "        indices, test_size=0.2, stratify=y_train_aug, random_state=42\n",
    "    )\n",
    "\n",
    "    embed_train_x = {k: v[embed_train_idx] for k, v in X_train_aug.items()}\n",
    "    embed_val_x   = {k: v[embed_val_idx] for k, v in X_train_aug.items()}\n",
    "    embed_y_train = y_train_aug[embed_train_idx]\n",
    "    embed_y_val   = y_train_aug[embed_val_idx]\n",
    "\n",
    "    model, inputs, pooled = get_embedding_model_multi_input(\n",
    "        input_embedding_dim=16,\n",
    "        final_embedding_dim=128,\n",
    "        vocab_sizes=vocab_sizes,\n",
    "        max_len=MAX_SEQUENCE_LENGTH,\n",
    "    )\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                restore_best_weights=True, start_from_epoch=20)\n",
    "    model.compile(optimizer='adam', loss=SupervisedContrastiveLoss())\n",
    "\n",
    "    model.fit(\n",
    "        x=embed_train_x, y=embed_y_train,\n",
    "        validation_data=(embed_val_x, embed_y_val),\n",
    "        epochs=100, batch_size=32, shuffle=True,\n",
    "        callbacks=[early_stop]\n",
    "    )\n",
    "\n",
    "    emb_test = model.predict(X_test, batch_size=32)\n",
    "    emb_train = model.predict(X_train, batch_size=32)\n",
    "    hit_rate, _ = top_k_retrieve_acc(emb_train, y_train, emb_test, y_test, unknown_threshold=unknown_vendor_threshold)\n",
    "    hit_rates.append(hit_rate)\n",
    "    print(f'Fold {fold} - Hit Rate: {hit_rate:.3f}')\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model, preprocessor\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Cross-Validation Summary\")\n",
    "print(f\"Average HR: {np.mean(hit_rates):.4f}  {np.std(hit_rates):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d060e4",
   "metadata": {},
   "source": [
    "## New Vendors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc34c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_vendors(dataset_list, unseen_vendors):\n",
    "    \"\"\"\n",
    "    Split the dataset into seen and unseen vendor groups.\n",
    "\n",
    "    Given a list of data points and a list of unseen vendor labels,\n",
    "    this function separates the dataset into:\n",
    "        - seen_raw:   samples whose vendor label is NOT in unseen_vendors\n",
    "        - unseen_raw: samples whose vendor label IS in unseen_vendors\n",
    "\n",
    "    This is used to hold out certain vendors entirely for unseen-vendor evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    seen_raw, unseen_raw = [], []\n",
    "    for dp in dataset_list:\n",
    "        vendor = dp.get(\"label\")\n",
    "        if vendor in unseen_vendors:\n",
    "            unseen_raw.append(dp)\n",
    "        else:\n",
    "            seen_raw.append(dp)\n",
    "    return seen_raw, unseen_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1297b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['low', 'medium', 'high']\n",
    "\n",
    "def categorize(value, q1, q2):\n",
    "    \"\"\"\n",
    "    Assign a numeric value into a category ('low', 'medium', 'high').\n",
    "\n",
    "    Given a value and two quantile thresholds (q1, q2):\n",
    "        - value <= q1  → 'low'\n",
    "        - value <= q2  → 'medium'\n",
    "        - otherwise    → 'high'\n",
    "\n",
    "    This is used to categorize vendors by their mean sequence length.\n",
    "    \"\"\"\n",
    "    \n",
    "    if value <= q1:\n",
    "        return categories[0]\n",
    "    elif value <= q2:\n",
    "        return categories[1]\n",
    "    else:\n",
    "        return categories[2]\n",
    "\n",
    "def sample_vendors_by_size(stats_df, n, size):\n",
    "    \"\"\"\n",
    "    Sample vendors from a specific size category.\n",
    "\n",
    "    Vendors are grouped into 'low', 'medium', or 'high' categories\n",
    "    based on their mean sequence lengths (using 33% and 66% quantiles).\n",
    "\n",
    "    This function:\n",
    "        1. Categorizes all vendors in stats_df.\n",
    "        2. Selects vendors from the requested category (size).\n",
    "        3. Randomly samples `n` vendors from that category.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_q1, mean_q2 = np.quantile(stats_df['mean_seq_len'], [0.33, 0.66])\n",
    "\n",
    "    stats_df = stats_df.copy()\n",
    "    stats_df['mean_cat'] = stats_df['mean_seq_len'].apply(lambda x: categorize(x, mean_q1, mean_q2))\n",
    "\n",
    "    groups = (\n",
    "        stats_df\n",
    "        .groupby(['mean_cat'])\n",
    "        .apply(lambda g: g['label'])\n",
    "    )\n",
    "    \n",
    "    if size not in groups:\n",
    "        raise ValueError(f\"No vendors in the '{size}' category.\")\n",
    "    \n",
    "    available_indices = list(groups[size])\n",
    "    if len(available_indices) < n:\n",
    "        raise ValueError(f\"Requested {n} samples but only {len(available_indices)} available in '{size}' group.\")\n",
    "    \n",
    "    sampled = sample(available_indices, n)\n",
    "    return sampled\n",
    "\n",
    "\n",
    "def sample_diverse_vendors(stats_df, n):\n",
    "    \"\"\"\n",
    "    Sample a diverse set of vendors across all size categories.\n",
    "\n",
    "    This function ensures balanced sampling across:\n",
    "        - 'low'\n",
    "        - 'medium'\n",
    "        - 'high'\n",
    "\n",
    "    It divides n approximately evenly across the three categories.\n",
    "    Any remainder vendors are distributed one-by-one starting from 'low'.\n",
    "\n",
    "    Returns a combined list of sampled vendor labels.\n",
    "    \"\"\"\n",
    "\n",
    "    base_n_per, remainder_n = n // len(categories), n % len(categories)\n",
    "    sampled = []\n",
    "    for category in categories:\n",
    "        to_sample = base_n_per\n",
    "\n",
    "        if remainder_n:\n",
    "            to_sample += 1\n",
    "            remainder_n -= 1\n",
    "        \n",
    "        sampled += sample_vendors_by_size(stats_df, to_sample, category)\n",
    "        \n",
    "    return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f012fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_vendor_dataset['seq_len'] = anon_vendor_dataset['profile'].apply(len)\n",
    "stats_df = anon_vendor_dataset.groupby('label')['seq_len'].mean().reset_index()\n",
    "stats_df.rename(columns={'seq_len': 'mean_seq_len'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_groups = []\n",
    "for i in range(5):\n",
    "   unseen_groups.append(sample_diverse_vendors(stats_df, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86034bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out_unseen(dataset_list, unseen_vendors, top_k_elements=30, top_k_events=90,\n",
    "                    epochs=50, patience=10, early_stop_start=20, test_size=0.25, augment=False, balance=False, max_len=0, unknown_vendor_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Perform hold-out evaluation for unseen vendors.\n",
    "\n",
    "    This function trains a supervised contrastive embedding model\n",
    "    ONLY on the vendors not included in unseen_vendors.\n",
    "\n",
    "    Workflow:\n",
    "        1. Split dataset → seen vendors (training) vs unseen vendors (held-out)\n",
    "        2. Preprocess data and build padded model inputs\n",
    "        3. Train embedding model on seen vendors\n",
    "          (with optional augmentation or class balancing)\n",
    "        4. Generate embeddings for:\n",
    "              - seen test data\n",
    "              - unseen vendor data\n",
    "        5. Evaluate two metrics:\n",
    "            a. Seen-vs-Unseen Detection Accuracy:\n",
    "                  Predict whether each unseen sample belongs to a new vendor\n",
    "                  by thresholding its max similarity to seen embeddings.\n",
    "            b. Unseen Pair Similarity Accuracy:\n",
    "                  Among unseen embeddings, check whether a sample’s nearest\n",
    "                  unseen neighbor has the same vendor label.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"seen_vs_unseen_acc\": float,\n",
    "            \"unseen_pair_similarity_acc\": float\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    seen_raw, unseen_raw = filter_vendors(dataset_list, unseen_vendors)\n",
    "\n",
    "    seen_labels = [dp[\"label\"] for dp in seen_raw]\n",
    "    idx_train, idx_test = train_test_split(\n",
    "        np.arange(len(seen_raw)),\n",
    "        test_size=test_size,\n",
    "        stratify=seen_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "    train_raw = [seen_raw[i] for i in idx_train]\n",
    "    test_raw  = [seen_raw[i] for i in idx_test]\n",
    "\n",
    "    preprocessor = ProfilePreprocessor(top_k_elements=top_k_elements,\n",
    "                                       top_k_events=top_k_events)\n",
    "    train_processed = preprocessor.fit_transform(train_raw)\n",
    "    test_processed  = preprocessor.transform(test_raw)\n",
    "    unseen_processed = preprocessor.transform(unseen_raw)\n",
    "\n",
    "    if not max_len:\n",
    "      max_len = max(len(dp[\"profile\"]) for dp in train_processed)\n",
    "    X_train, y_train, _ = prepare_model_inputs(train_processed, max_len=max_len)\n",
    "    X_test, y_test, _   = prepare_model_inputs(test_processed,  max_len=max_len)\n",
    "    X_unseen, y_unseen, _ = prepare_model_inputs(unseen_processed, max_len=max_len)\n",
    "\n",
    "    vocab_sizes = {\n",
    "        'behavior': preprocessor.behavior_vocab_size,\n",
    "        'method': preprocessor.method_vocab_size,\n",
    "        'element_tagname': preprocessor.element_tagname_vocab_size,\n",
    "        'event_handled': preprocessor.event_handled_vocab_size,\n",
    "    }\n",
    "\n",
    "    model, inputs, pooled = get_embedding_model_multi_input(\n",
    "          input_embedding_dim=16,\n",
    "          final_embedding_dim=128,\n",
    "          vocab_sizes=vocab_sizes,\n",
    "          max_len=max_len,\n",
    "      )\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=patience,\n",
    "                               restore_best_weights=True, start_from_epoch=early_stop_start)\n",
    "    model.compile(optimizer='adam', loss=SupervisedContrastiveLoss())\n",
    "\n",
    "    if augment:\n",
    "      print('Augmenting')\n",
    "      X_train_aug, y_train_aug = augment_data(X_train, y_train, min_len=5, middle=augment == 'middle')\n",
    "\n",
    "      model.fit(X_train_aug, y_train_aug, validation_data=(X_test, y_test),\n",
    "                epochs=epochs, callbacks=[early_stop], batch_size=32)\n",
    "    elif balance:\n",
    "      print('Balancing')\n",
    "      X_train_aug, y_train_aug = balance_data(X_train, y_train, min_points=10, min_len=5, middle=balance == 'middle')\n",
    "\n",
    "      model.fit(X_train_aug, y_train_aug, validation_data=(X_test, y_test),\n",
    "                epochs=epochs, callbacks=[early_stop], batch_size=32)\n",
    "    else:\n",
    "      model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "                epochs=epochs, callbacks=[early_stop], batch_size=32)\n",
    "\n",
    "    embedding_model = tf.keras.Model(inputs=inputs, outputs=pooled)\n",
    "    emb_test   = embedding_model.predict(X_test)\n",
    "    emb_unseen = embedding_model.predict(X_unseen)\n",
    "\n",
    "    # Seen-vs-unseen detection\n",
    "    sims_max = [np.max(cosine_similarity(e.reshape(1, -1), emb_test))\n",
    "                for e in emb_unseen]\n",
    "    predicted_unseen_mask = np.array(sims_max) < unknown_vendor_threshold\n",
    "    true_unseen_mask = np.ones_like(predicted_unseen_mask, dtype=bool)\n",
    "    seen_vs_unseen_acc = np.mean(predicted_unseen_mask == true_unseen_mask)\n",
    "\n",
    "    # Unseen pair similarity accuracy\n",
    "    correct, total = 0, 0\n",
    "    for i in range(len(emb_unseen)):\n",
    "        sims = cosine_similarity(emb_unseen[i].reshape(1, -1), emb_unseen)[0]\n",
    "        sims[i] = -1\n",
    "        j = np.argmax(sims)\n",
    "        if y_unseen[i] == y_unseen[j] and sims[j] >= unknown_vendor_threshold:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    unseen_pair_acc = correct / total if total > 0 else 0\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Seen-vs-Unseen Detection Accuracy: {seen_vs_unseen_acc:.4f}\")\n",
    "    print(f\"Unseen Pair Similarity Accuracy: {unseen_pair_acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"seen_vs_unseen_acc\": seen_vs_unseen_acc,\n",
    "        \"unseen_pair_similarity_acc\": unseen_pair_acc\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ad429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluate unseen-vendor performance across multiple unseen vendor groups.\n",
    "\n",
    "Each entry in unseen_groups is a list of vendor labels that will be held out\n",
    "entirely during training. For each group:\n",
    "\n",
    "    1. Run hold-out evaluation using hold_out_unseen()\n",
    "    2. Train the model on the remaining (seen) vendors\n",
    "    3. Compute:\n",
    "         - seen_vs_unseen_acc: ability to detect unseen vendors\n",
    "         - unseen_pair_similarity_acc: how well embeddings cluster unseen vendors\n",
    "\n",
    "We store both metrics across all unseen vendor groups and then compute:\n",
    "\n",
    "    - Mean and standard deviation of seen-vs-unseen detection accuracy\n",
    "    - Mean and standard deviation of unseen vendor pair similarity accuracy\n",
    "\n",
    "This provides a robust measurement of how well the model generalizes\n",
    "to completely new, never-before-seen vendors.\n",
    "\"\"\"\n",
    "\n",
    "seen_vs_unseen_accs = []\n",
    "unseen_pair_sim_accs = []\n",
    "for unseen_vendors in unseen_groups:\n",
    "    results = hold_out_unseen(dataset_list, unseen_vendors,\n",
    "                                epochs=epochs, patience=patience, early_stop_start=early_stop_start, augment=augment, balance=balance, top_k_elements=top_k_elements, top_k_events=top_k_events, unknown_vendor_threshold=unknown_vendor_threshold)\n",
    "    seen_vs_unseen_accs.append(results[\"seen_vs_unseen_acc\"])\n",
    "    unseen_pair_sim_accs.append(results[\"unseen_pair_similarity_acc\"])\n",
    "\n",
    "print('Seen Vs. Unseen Detection')\n",
    "print(f\"Mean: {np.mean(seen_vs_unseen_accs):.4f}, Std: {np.std(seen_vs_unseen_accs):.4f}\")\n",
    "print('Unseen Pairs Accuracy')\n",
    "print(f\"Mean: {np.mean(unseen_pair_sim_accs):.4f}, Std: {np.std(unseen_pair_sim_accs):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
